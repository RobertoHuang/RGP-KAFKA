# Kafka实战



## 关于Kafka

`Apache Kafka`是一种分布式的、基于发布/订阅的消息系统，由`Scala`语言编写。以下是`Kafka`主要特点

- `Kafka`具有近乎实时性的消息处理能力，即使面对海量消息也能够高效地存储消息和查询消息
- `Kafka`支持批量读写消息，并且会对消息进行压缩，这样既提高了网络利用率也提高了压缩率
- `Kafka`支持消息分区，每个分区中的消息保证顺序传输，而分区之间则可以并发操作提高了并发能力
- `Kafka`支持在线增加分区，支持在线水平扩展，支持多副本机制提高数据容灾能力

`Apache Kafka`与其他消息中间件的对比【[消息中间件选型分析:从Kafka与RabbitMQ的对比看全局](https://www.infoq.cn/article/kafka-vs-rabbitmq?utm_source=infoq&utm_medium=popular_widget&utm_campaign=popular_content_list&utm_content=homepage)】

## Kafka核心概念

`Kafka`的核心概念比较多，一言以蔽之是不大可能。若只是使用`Kafka API`可先了解如下概念即可

- `Topic`:消息的订阅和发送都是基于`Topic`，它像一个特定主题的收件箱(`Producer`往里丢`Consumer`取出)
- `Partition`:大多数消息系统同一个`Topic`下的消息存储在一个队列中，分区的概念就是把这个队列划分为若干个小队列，每一个小队列就是一个分区。分区之间可以并发操作提高了并发能力
- `Offset`:数据消费的偏移量，它是一个消息在一个`Partition`中的唯一标示，表示自己的消息顺序
- `Group`:消费组(为什么要有消费组的概念？队列都有单播、广播。广播在`Kafka`中就是让多个`Consumer`同时消费同一条消息的时候才用不同的`Group`、单播就是让多个`Consumer`在消费一条消息时使用一个`Group`)，同一个`Topic`的数据可以被多个`Group`消费，相互间不受影响。一个组内消费完数据不会重新消费

关于`Kafka`基础概念可参考官方文档:[Kafka Introduction - Kafka简介](http://kafka.apache.org/intro.html)

## Kafka消费者实战

- 引入`kafka-clients`依赖

  ```xml
  <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka-clients</artifactId>
  </dependency>
  ```

- 入门 - 最简单的消费者`Demo`

  ```java
  public class ConsumerTest {
      private static final String TOPIC = "topic-test";
      private static final String GROUP_ID = "group_id";
      private static final String CLIENT_ID = "client_id";
      private static final String BOOTSTRAP = "localhost:9092";
  
      public static void main(String[] args) {
          Properties properties = new Properties();
          // brokers地址
          properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP);
          // 表示consumer所在的组
          properties.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
          // 表示客户端ID 可与应用名相同
          properties.put(ConsumerConfig.CLIENT_ID_CONFIG, CLIENT_ID);
          // 是否自动提交Offset
          properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
          // 自动提交Offset时间间隔
          properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
          // 数据传输反序列化方式
          properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
          properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
          //  设置消费从最新的开始消费，可选值latest, earliest, none
          properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
  
          KafkaConsumer<String, String> consumer = new KafkaConsumer<>(properties);
          Collection<String> topics = Arrays.asList(TOPIC);
          // assign跟subscribe区别：用户已经指定了消费分区，如果有消费者加入或退出，assign不会进行reblance
          // subscribe有个重载函数subscribe(Pattern pattern, ConsumerRebalanceListener listener)可用于监听reblance 有兴趣的同学可自行尝试
          // assign为consumer手动、显示的指定需要消费的topic-partitions，不受group.id限制，相当与指定的group无效
          // subscribe为consumer自动分配partition，有内部算法保证topic-partition以最优的方式均匀分配给同group下的不同consumer
          consumer.subscribe(topics);
          while (true) {
              ConsumerRecords<String, String> records = consumer.poll(100);
              records.forEach(record -> System.out.printf("client :%s , topic:%s , parititon :%d ,offset = %d  ,key =%s , value =%s%n", CLIENT_ID, record.topic(), record.partition(), record.offset(), record.key(), record.value()));
          }
      }
  }
  ```

- 入门 - 手动提交`Offset Demo`

  - 同步方式`SYNC`

    ```java
    consumer.commitSync(Map<TopicPartition, OffsetAndMetadata> offsets)
    ```

  - 异步方式`ASYNC`

    ```java
    1.consumer.commitAsync()
        
    2.consumer.commitAsync(OffsetCommitCallback callback)
    
    3.consumer.commitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, callback)
    ```

  刚接触`Kafka`的同学可能不理解为什么要提交`Offset`，提交`Offset`主要是告知`Broker`哪些消息已经被消费

- 数据回溯消费 - 从指定`Offset`开始消费

  ```reStructuredText
  1.Kafka为数据回溯消费提供两种方式，分别是Offset和Timestamp
  
  2.如果已知Offset则直接使用consumer.seek(TopicPartition partition, long offset)指定
  
  3.如果提供Timestamp则需要根据Timestamp找到对应的Offset再调用上述API，方法如下
      Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
      timestampsToSearch.put(new TopicPartition(topic, 0), System.currentTimeMillis() -10*1000);
      Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(timestampsToSearch);
  ```

关于`Consumer`的所有配置参数可在`ConsumerConfig`中找到，配置说明可参考:[Kafka Consumer Config](http://kafka.apache.org/0101/documentation.html#newconsumerconfigs)

## Kafka生产者实战

- 引入`kafka-clients`依赖

  ```xml
  <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka-clients</artifactId>
  </dependency>
  ```

- 入门 - 最简单的生产者`Demo`

  ```java
  public class ProducerTest {
      private static final String TOPIC = "topic-test";
      private static final String BOOTSTRAP = "localhost:9092";
  
      public static void main(String[] args) throws InterruptedException {
          Properties properties = new Properties();
          // brokers地址
          properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP);
          // 数据传输序列化方式
          properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
          properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
          // acks - 0 发出数据不进行ACK确认消息是否发送成功
          // acks - 1 当Leader已经确认这条消息已经写入到日志文件，但是并没有等待所有的follower进行数据同步。这钟情况下会有少数情况数据丢失
          // acks - all 这跟上面一种情况相似，当所有的除了leader还有其他的ISR集合中的follower都成功将数据的写入文件。这种情况下数据可靠性最强
          properties.setProperty(ProducerConfig.ACKS_CONFIG, "1");
          // 消息批次大小
          properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, "10000");
          // 消息发送缓冲区大小
          properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 1 * 1024 * 1024);
          // 如果消息发送失败，最多进行重试多少次
          properties.setProperty(ProducerConfig.RETRIES_CONFIG, "3");
          KafkaProducer<Object, Object> kafkaProducer = new KafkaProducer<>(properties);
          for (int i = 0; i < 100; i++) {
              kafkaProducer.send(new ProducerRecord<>(TOPIC, Integer.toString(i), "message-" + i), (metadata, exception) -> System.out.printf(" msg ==> partition :%d  , offset :%d ,topic :%s , valuesize: %d %n", metadata.partition(), metadata.offset(), metadata.topic(), metadata.serializedValueSize()));
          }
          Thread.sleep(100000);
      }
  }
  ```

- 生产者实战拦截器 - 对消息进行前后置处理

  ```reStructuredText
  1.实现ProducerInterceptor接口，实现对应方法
  
  2.为生产者添加配置属性 如:properties.put("interceptor.classes", xxx.class.getName());
  ```

- 将消息按照一定规则路由到指定`Partition`

  ```reStructuredText
  1.实现Partitioner接口重写对应方法
  
  2.为生产者添加配置属性 如:properties.put("partitioner.class", xxx.class.getName());
  ```

- `Kafka`集成`Protostuff`进行序列化

  自定义`Kafka`序列化方式可参考:[kafka自定义消息序列化和反序列化方式](https://blog.csdn.net/shirukai/article/details/82152172)

关于`Producer`的所有配置参数可在`ProducerConfig`中找到，配置说明可参考:[Kafka Producer Config](http://kafka.apache.org/0101/documentation.html#producerconfigs)

## Spring整合Kafka

- 引入`Maven`依赖

  ```xml
  <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka-clients</artifactId>
  </dependency>
  
  <dependency>
      <groupId>org.springframework.kafka</groupId>
      <artifactId>spring-kafka</artifactId>
      <exclusions>
          <exclusion>
              <groupId>org.apache.kafka</groupId>
              <artifactId>kafka-clients</artifactId>
          </exclusion>
      </exclusions>
  </dependency>
  ```

  注:引入的`spring-kafka`版本在`2.0`或者`2.X`时`Spring`版本在`5.0`才能支持

- 写`Spring`的`Demo`有点繁琐，可自行参考[Spring for Apache Kafka](https://docs.spring.io/spring-kafka/reference/htmlsingle/)开始吧

- `SpringBoot`整合`Kafka`可参考`Demo`:[SpringBoot整合Kafka Demo](https://github.com/lilaizhencn/spring-boot-kafka-demo)，相对比较简单就不展开写啦